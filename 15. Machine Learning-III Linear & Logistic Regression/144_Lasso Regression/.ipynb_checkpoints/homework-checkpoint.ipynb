{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51908383",
   "metadata": {},
   "source": [
    "### Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
    "### Q2. What is the main advantage of using Lasso Regression in feature selection?\n",
    "### Q3. How do you interpret the coefficients of a Lasso Regression model?\n",
    "### Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?\n",
    "### Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
    "### Q6. What is the difference between Ridge Regression and Lasso Regression?\n",
    "### Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
    "### Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d60fa0",
   "metadata": {},
   "source": [
    "### 1. Lasso Regression:\n",
    "\n",
    "    Lasso Regression (Least Absolute Shrinkage and Selection Operator) is a regression technique that, like Ridge Regression, introduces a penalty term to the ordinary least squares (OLS) objective function. However, it uses the L1 regularization penalty, which adds the absolute values of the coefficients (λ * sum of absolute values of coefficients) to the loss function. Lasso tends to produce sparse models by pushing some coefficients to exactly zero, effectively performing feature selection.\n",
    "    \n",
    "    Difference from Other Regression Techniques:\n",
    "        The primary difference lies in the penalty term used for regularization. Lasso uses L1 regularization,\n",
    "        whereas Ridge Regression uses L2 regularization. Lasso tends to produce sparse solutions, effectively\n",
    "        performing feature selection by zeroing out some coefficients.\n",
    "###  2. Advantage of Lasso Regression in Feature Selection:\n",
    "    \n",
    "    The main advantage of Lasso Regression in feature selection is its ability to shrink coefficients to zero, effectively performing automatic feature selection. This is particularly useful when dealing with datasets containing a large number of features, as it can help identify the most important predictors by discarding less relevant ones.\n",
    "\n",
    "    \n",
    "### 3. Interpretation of Lasso Regression Coefficients:\n",
    "   \n",
    "    The coefficients in Lasso Regression, similar to other regression techniques, represent the change in the\n",
    "    response variable corresponding to a one-unit change in the predictor variable, holding other variables\n",
    "    constant. However, due to the nature of L1 regularization, some coefficients might be exactly zero, indicating\n",
    "    that these predictors have been excluded from the model.\n",
    "\n",
    "\n",
    "### 4. Tuning Parameters in Lasso Regression:\n",
    "    \n",
    "    The primary tuning parameter in Lasso Regression is λ (lambda), which controls the strength of regularization.\n",
    "    Higher values of λ increase the penalty on the coefficients, leading to more coefficients being pushed towards\n",
    "    zero or set to zero. The choice of λ influences the sparsity of the model and impacts the trade-off between\n",
    "    bias and variance.\n",
    "\n",
    "\n",
    "### 5. Lasso Regression for Non-linear Problems:\n",
    "    \n",
    "    Lasso Regression is inherently a linear model, so it's more suitable for linear regression problems. However,\n",
    "    in practice, it's possible to combine Lasso Regression with non-linear transformations of features to address\n",
    "    non-linearity in the data.\n",
    "\n",
    "\n",
    "### 6. Difference between Ridge and Lasso Regression:\n",
    "    \n",
    "    The primary difference lies in the penalty term used. Ridge uses L2 regularization, which generally leads to\n",
    "    shrinking coefficients but not to zero, while Lasso uses L1 regularization, resulting in sparsity by forcing\n",
    "    some coefficients to zero.\n",
    "\n",
    "\n",
    "### 7. Handling Multicollinearity:\n",
    "    \n",
    "    Lasso Regression handles multicollinearity by reducing the coefficients of correlated variables, effectively\n",
    "    selecting one variable over others by zeroing out their coefficients. This feature makes it particularly useful\n",
    "    when dealing with multicollinear datasets.\n",
    "\n",
    "\n",
    "### 8. Choosing the Optimal λ in Lasso Regression:\n",
    "    \n",
    "    Cross-validation techniques, such as k-fold cross-validation, are commonly used to select the optimal value of\n",
    "    λ in Lasso Regression. The value of λ that minimizes the prediction error on unseen data is typically chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f524f5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
